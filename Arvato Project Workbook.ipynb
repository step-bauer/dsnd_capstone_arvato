{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\r\n",
    "\r\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\r\n",
    "\r\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview and Table of Content\r\n",
    "\r\n",
    "The notebook will implement this workflow:\r\n",
    "\r\n",
    "1. [Prepare jupyter lab notebook environment](#Prepare-jupyter-lab-notebook-environment)\r\n",
    "1. [Download Data](#Download-Data)\r\n",
    "1. [Data exploration and cleaning](#Data-exploration-and-cleaning)\r\n",
    "    1. explore the data\r\n",
    "    1. cleaning (handling null and empty values, unknown values, encode categorical values)\r\n",
    "1. [Visualizations](#Visualizations)\r\n",
    "    1. correlation studies\r\n",
    "1. [Data preparation and transformation](#Data-preparation-and-transformation)\r\n",
    "    1. Feature engineering (PCA)\r\n",
    "1. [Model development and training](Model-development-and-training)\r\n",
    "    1. Develop a model\r\n",
    "    1. Train a model\r\n",
    "    1. Model validation and evaluation\r\n",
    "    1. Hyperparameters tuning\r\n",
    "    1. Select the best performing model based on the test results\r\n",
    "1. [Deploy model](#Deploy-Model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare jupyter lab notebook environment\r\n",
    "---\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auto Reload Modules \r\n",
    "configure auto-reload of modules when they have been changed - this simplifies developing and testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python --version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update Conda Packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! conda update -y conda"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! conda update --all -y"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! conda install pyarrow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! conda install -y -c anaconda progressbar2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! conda list"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and global configs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import progressbar\r\n",
    "\r\n",
    "# magic word for producing visualizations in notebook\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# display the N columns and rows\r\n",
    "pd.set_option('display.max_columns', 50)\r\n",
    "\r\n",
    "pd.set_option('display.max_rows', 100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activate intelex for scikit\n",
    "see [activate intelex for scikit](https://intel.github.io/scikit-learn-intelex/index.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#! conda install -y scikit-learn-intelex"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download Data\r\n",
    "---\r\n",
    "The four data sets\r\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\r\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\r\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\r\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\r\n",
    "    \r\n",
    "and two files of description    \r\n",
    "- `DIAS Attributes - Values 2017.xlsx`\r\n",
    "- `DIAS Information Levels - Attributes 2017.xlsx`\r\n",
    "\r\n",
    "can be downloaded from the Udacity project workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data exploration and cleaning\n",
    "---\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data from S3\r\n",
    "The load script assumes that the downloaded data has been transferred to S3.\r\n",
    "\r\n",
    "The data load of the AZDIAS data set takes more than a minute the CUSTOMERS data set should be loaded in less than 20 secs\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "if os.path.exists('data') and os.path.isdir('data'):\r\n",
    "    prefix = './data'\r\n",
    "else:\r\n",
    "    prefix = 's3://sagemaker-eu-central-1-292575554790/dsnd_arvato'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! aws s3 ls s3://sagemaker-eu-central-1-292575554790/dsnd_arvato/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "df_azdias = pd.read_csv(f'{prefix}/Udacity_AZDIAS_052018.csv', sep=';', index_col='LNR')\r\n",
    "# load in the data\r\n",
    "#azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';')\r\n",
    "#customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "df_customers = pd.read_csv(f'{prefix}/Udacity_CUSTOMERS_052018.csv', sep=';', index_col='LNR')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_customers.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checks \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data errors during load\r\n",
    "during the load process we got two error messages for columns 18 and 19. I will check this here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# column 18 has 0-based index 17\r\n",
    "df_azdias.iloc[:,17].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# column 19 has 0-based index 18\r\n",
    "df_customers.iloc[:,18].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias.columns[17:19]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "**Result:** The errors is caused by string values \"X\" and \"XX\" in the datasets in columns CAMEO_DEUG_2015 and CAMEO_INTL_2015.\r\n",
    "\r\n",
    "I will add some code to the Data Cleaner class to handle this\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### check for duplicates\r\n",
    "check if dataset contains duplicate records based on column ID LNR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias.index.duplicated().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_customers.index.duplicated().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading & Explore Metadata\r\n",
    "\r\n",
    "### Load Metadata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_metadata = pd.read_excel(f'{prefix}/DIAS Attributes - Values 2017.xlsx', usecols='B:E', dtype='str', header=1).fillna(method='ffill')\r\n",
    "df_metadata.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explore Metadata\r\n",
    "\r\n",
    "1. check nulls\r\n",
    "2. check unkown values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_metadata.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"Number of unique attributes: {df_metadata['Attribute'].unique().shape[0]}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"Number of Attributes that can be unnkown value: {df_metadata['Meaning'].str.contains('unknown').sum()}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"Number of Attributes that can be `no transaction known` value: {df_metadata['Meaning'].str.contains('no transaction.? known', regex=True).sum()}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"Total: {df_metadata['Meaning'].str.contains('unknown').sum() + df_metadata['Meaning'].str.contains('no transaction.? known', regex=True).sum()}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare dataset features (columns)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# columns that customers dataset contain but azidas not\r\n",
    "set(df_customers.columns) - set(df_azdias.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# columns that azdias dataset contain but customers not\r\n",
    "set(df_azdias.columns) - set(df_customers.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Result**: `CUSTOMERS` dataset has 3 more columns {'CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metadata Columns compared to Dataset columns\r\n",
    "\r\n",
    "check for which columns of the dataset a metadata description exists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_metadata_cols = df_metadata['Attribute'].copy()\r\n",
    "# some columns of the metadata ends on _RZ whereas the datasets have the same columns whcih do not end on _RZ\r\n",
    "# therefore we replace this\r\n",
    "df_metadata_cols = df_metadata_cols.str.replace('_RZ','')\r\n",
    "\r\n",
    "diff_set = set(df_azdias.columns) - set(df_metadata_cols)\r\n",
    "print(f'number of cols in AZDIAS dataset but not described in Metadata: {len(diff_set)}')\r\n",
    "pd.Series(list(diff_set)).sort_values().unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "diff_set2 = set(df_metadata_cols) - set(df_azdias.columns)\r\n",
    "print(f'number of cols in Metadata but not in AZDIAS dataset: {len(diff_set2)}')\r\n",
    "diff_set2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias[list(diff_set)].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract `Kinder` information and build new feature\r\n",
    "we use the ANZ_KINDER and ALTER_KIND(n) columns to derive the number of children <10 and >= 10"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_moreThan4Children = df_azdias[df_azdias['ANZ_KINDER']>4].shape[0]\r\n",
    "num_withChildren = df_azdias[df_azdias['ANZ_KINDER']>0].shape[0]\r\n",
    "df_children5plus = df_azdias[(df_azdias['ANZ_KINDER']>4) & (df_azdias['ALTER_KIND4']<10)].filter(regex='(ANZ_KINDER)|(ALTER_KIND.?)')\r\n",
    "\r\n",
    "print(f'number of records with more than 4 children: {num_moreThan4Children} of {df_azdias.shape[0]:,.0f} ({(num_moreThan4Children / df_azdias.shape[0] *100):6.5f} %)')\r\n",
    "print(f'number of records with at least one child: {num_withChildren} of {df_azdias.shape[0]:,.0f} ({(num_withChildren / df_azdias.shape[0] *100):6.5f} %)')\r\n",
    "print(f'number of records with ANZ_KINDER >= 5 and ALTER_KIND4 < 10: {df_children5plus.shape[0]}\\n')\r\n",
    "print('-'*80)\r\n",
    "\r\n",
    "ax = df_azdias['ANZ_KINDER'].plot.hist()\r\n",
    "ax.set_yscale('log')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Result** The query above shows that there just 9 records with more or equal than 5 children and an age of child4 (`ALTER_KIND4`) < 10. In addition the `ALTER_KIND` column values are ordered so we can assume that the age of child5 and higher is >= 10"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_moreThan4Children = df_customers[df_customers['ANZ_KINDER']>4].shape[0]\r\n",
    "num_withChildren = df_customers[df_customers['ANZ_KINDER']>0].shape[0]\r\n",
    "df_children5plus = df_customers[(df_customers['ANZ_KINDER']>4) & (df_customers['ALTER_KIND4']<10)].filter(regex='(ANZ_KINDER)|(ALTER_KIND.?)')\r\n",
    "\r\n",
    "print(f'number of records with more than 4 children: {num_moreThan4Children} of {df_customers.shape[0]:,.0f} ({(num_moreThan4Children / df_customers.shape[0] *100):6.5f} %)')\r\n",
    "print(f'number of records with at least one child: {num_withChildren} of {df_customers.shape[0]:,.0f} ({(num_withChildren / df_customers.shape[0] *100):6.5f} %)')\r\n",
    "print(f'number of records with ANZ_KINDER >= 5 and ALTER_KIND4 < 10: {df_children5plus.shape[0]}\\n')\r\n",
    "print('-'*80)\r\n",
    "\r\n",
    "max_children = int(df_customers['ANZ_KINDER'].max())\r\n",
    "ax = df_customers['ANZ_KINDER'].plot.hist(bins=range(max_children+2), width=0.8)\r\n",
    "ax.set_yscale('log')\r\n",
    "ax.set_xticks(range(0,max_children+1));\r\n",
    "ax.set_title('Record distribution for number of children');\r\n",
    "ax.set_xlabel('number of children')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metadata Summary\n",
    "\n",
    "The value *\"unkown\"* will be treated like a missing value.\n",
    "\n",
    "The value *\"no transaction(s) known\"* will be treated as if the customer has done no transaction\n"
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class Cleaner\r\n",
    "----"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code for the ETL Pipeline is outsourced to python module ==> see python module etl.processor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import python.etl.processor as etlp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning \r\n",
    "---\r\n",
    "\r\n",
    "The `DataCleaner` class will handle the following:\r\n",
    "\r\n",
    "* replace `unknown` values (represented by -1, 0, 9 see [Metadata Descriptions](#Loading-and-Explore-Metadata))\r\n",
    "* handle the errors raised during the load\r\n",
    "* handle categorical variables\r\n",
    "* drop not needed columns\r\n",
    "\r\n",
    "see sections below for details"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TESTING = False\r\n",
    "if TESTING:\r\n",
    "    df_azdias_cleaned = df_azdias.iloc[:100,:].copy()\r\n",
    "else:\r\n",
    "    df_azdias_cleaned = df_azdias.copy()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handle Unknown / Missing Data\r\n",
    "\r\n",
    "The dataset contains a lot of unkown values. Many times these values are encoded by -1, 0 or 9 (see Metadata files). I replace all unkown values by np.NaN to use standard pandas function for imputinig and dropping.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Assess missing data in columns\r\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\r\n",
    "df_nulls = df_azdias.isnull().sum(axis=0)\r\n",
    "ax.hist(df_nulls, bins =50, alpha=0.5)\r\n",
    "ax_bis = ax.twinx()\r\n",
    "ax_bis.hist(df_nulls, bins =50, cumulative=True, density=True, histtype='step', color='red', alpha=0.8, label='cum_line')\r\n",
    "#ax_bis.hist(df_azdias_cleaned.isnull().sum(axis=0), bins =50, cumulative=-1, density=True, histtype='step', color='red', alpha=0.95)\r\n",
    "plt.title('Distributions of missing data before cleaning')\r\n",
    "ax.set_xlabel('# Missing Values (NaN)')\r\n",
    "ax.set_ylabel('Columns');\r\n",
    "ax_bis.set_ylabel('cumulative');\r\n",
    "ax_bis.hlines(xmin=0, xmax=df_nulls.max(), y=0.9, linestyles='dashed', color='grey', label='0.9')\r\n",
    "ax_bis.legend(bbox_to_anchor=(1.07, 1.0), loc='upper left');\r\n",
    "\r\n",
    "plt.savefig('dist_of_missingdata_before_transformation.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most columns have less than 25% missing values. Some columns have more than 50% missing data. Let's find them"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_of_records = df_azdias_cleaned.shape[0]\r\n",
    "s_missing_data = df_azdias_cleaned.isnull().sum(axis=0)\r\n",
    "s_missing_data_pct = df_azdias_cleaned.isnull().sum(axis=0) / num_of_records \r\n",
    "\r\n",
    "df_missing_data = pd.DataFrame({'abs':s_missing_data,'pct':s_missing_data_pct})\r\n",
    "df_missing_data.sort_values(by='pct', ascending=False)[:20]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Results**: \r\n",
    "* There 19 variables with more than 25% missing values -> These are candidates to drop\r\n",
    "* There are some variables that have all the same number of missing data (257113 - D19_...).\r\n",
    "* the variables `ALTER_KIND1` - `ALTER_KIND4` have a huge number of missing values. This is because they are dependent on `ANZ_KINDER` (number of children) so that for all records with `ANZ_KINDER`=0 the values for `ALTER_KIND1`- `ALTER_KIND4` are missing. We will handle this in feature engineering part and build a new varaible for these\r\n",
    "\r\n",
    "\r\n",
    "**Note**: The drop operation will be the last part as columns maybe needed during the feature engineering process\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "drop_level = 0.25\r\n",
    "columns_to_drop = s_missing_data_pct.sort_values(ascending=False)\r\n",
    "columns_to_drop = columns_to_drop[columns_to_drop>drop_level].index\r\n",
    "columns_to_drop"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inverstigate  columns that throw an error\r\n",
    "Info: just a copy from above [Data error during load](#Data-errors-during-load)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned['CAMEO_DEUG_2015'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obviously the 'X' is causing the issue. I will replace this by np.NaN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned['CAMEO_INTL_2015'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obviously the 'XX' is causing the issue. I will replace this by np.NaN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handle Categorical Values\r\n",
    "\r\n",
    "The datasets have a huge number of categorical variables. Most of the categorical variables are already encoded by int and floats, e.g. `AGER_TYP` is encoded by\r\n",
    "\r\n",
    "|value  | meaning |\r\n",
    "|-----  |---------|\r\n",
    "|-1     |\tunknown |\r\n",
    "|0\t    | no classification possible |\r\n",
    "|1\t    | passive elderly |\r\n",
    "|2\t    | cultural elderly |\r\n",
    "|3\t    | experience-driven elderly |\r\n",
    "\r\n",
    "We keep this encoding as in many cases the categorical values are Ordinal and just some nominal, e.g.\r\n",
    "\r\n",
    "variable `D19_TELKO_ANZ_12` the values are ordered from `very low activity` to `very high activity`\r\n",
    "\r\n",
    "|value  | meaning |\r\n",
    "|-----  |---------|\r\n",
    "|0      | no transactions known            |\r\n",
    "|1      | very low activity                |\r\n",
    "|2      | low activity                     |\r\n",
    "|3      | slightly increased activity      |\r\n",
    "|4      | increased activity               |\r\n",
    "|5      | high activity                    |\r\n",
    "|6      | very high activity               |\r\n",
    "\r\n",
    "\r\n",
    "However, some columns are of type = object. These are now investigated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.select_dtypes(include='object').head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Results Categorical:\r\n",
    "\r\n",
    "|variable   | type      | action    |\r\n",
    "|--         |--         | ---   \t|\r\n",
    "|CAMEO_DEU_2015| nominal | replace by one hot encoding |\r\n",
    "|D19_LETZTER_KAUF_BRANCHE | nominal | replace by one hot encoding |\r\n",
    "| EINGEFUEGT_AM | date | drop - this is just the date when the record has been added |\r\n",
    "| OST_WEST_KZ | nominal | replace by binary 0 and 1 |\r\n",
    "\r\n",
    "`CAMEO_DEUG_2015` encoded categorical variable - contains invalid strings 'X'\r\n",
    "`CAMEO_INTL_2015` encoded categorical variable - contains invalid strings 'XX'\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.set_option('max_seq_items',450)\r\n",
    "df_azdias_cleaned.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the cleaning process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dfCleaner = etlp.PreDataCleaner(df_metadata)\r\n",
    "\r\n",
    "df_azdias_cleaned = dfCleaner.transform(df_azdias_cleaned)\r\n",
    "df_azdias_cleaned.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check distribution of Missing values again"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Assess missing data in columns\r\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\r\n",
    "df_nulls = df_azdias.isnull().sum(axis=0)\r\n",
    "ax.hist(df_nulls, bins =50, alpha=0.5)\r\n",
    "ax_bis = ax.twinx()\r\n",
    "ax_bis.hist(df_nulls, bins =50, cumulative=True, density=True, histtype='step', color='red', alpha=0.8, label='cum_line')\r\n",
    "#ax_bis.hist(df_azdias_cleaned.isnull().sum(axis=0), bins =50, cumulative=-1, density=True, histtype='step', color='red', alpha=0.95)\r\n",
    "plt.title('Distributions of missing data before cleaning')\r\n",
    "ax.set_xlabel('# Missing Values (NaN)')\r\n",
    "ax.set_ylabel('Columns');\r\n",
    "ax.set_ylim(0, 200)\r\n",
    "ax_bis.set_ylabel('cumulative');\r\n",
    "ax_bis.hlines(xmin=0, xmax=df_nulls.max(), y=0.9, linestyles='dashed', color='grey', label='0.9')\r\n",
    "ax_bis.legend(bbox_to_anchor=(1.07, 1.0), loc='upper left');\r\n",
    "\r\n",
    "plt.savefig('dist_of_missingdata_after_transformation.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Assess missing data in columns\r\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\r\n",
    "df_nulls = df_azdias_cleaned.isnull().sum(axis=0)\r\n",
    "ax.hist(df_nulls, bins =50, alpha=0.5)\r\n",
    "ax_bis = ax.twinx()\r\n",
    "ax_bis.hist(df_nulls, bins =50, cumulative=True, density=True, histtype='step', color='red', alpha=0.8, label='cum_line')\r\n",
    "#ax_bis.hist(df_azdias_cleaned.isnull().sum(axis=0), bins =50, cumulative=-1, density=True, histtype='step', color='red', alpha=0.95)\r\n",
    "plt.title('Distributions of missing data after cleaning')\r\n",
    "ax.set_xlabel('# Missing Values (NaN)')\r\n",
    "ax.set_ylabel('Columns');\r\n",
    "ax.set_ylim(0, 200)\r\n",
    "ax_bis.set_ylabel('cumulative');\r\n",
    "ax_bis.hlines(xmin=0, xmax=df_nulls.max(), y=0.9, linestyles='dashed', color='grey', label='0.9')\r\n",
    "ax_bis.legend(bbox_to_anchor=(1.07, 1.0), loc='upper left');\r\n",
    "\r\n",
    "plt.savefig('dist_of_missingdata_after_transformation.jpg')\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison of distributions of missing data\r\n",
    "\r\n",
    "![alt distribution-before-transformation](dist_of_missingdata_before_transformation.jpg)\r\n",
    "![alt distribution-after-transformation](dist_of_missingdata_after_transformation.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Results**: \r\n",
    "\r\n",
    "* there is a significant increase of columns with no missing data  \r\n",
    "This is because of the transformation of categorical features to one-hot encoded columns. Therfore the number of columns increased with no missing values.\r\n",
    "* the other changes are becuse we replaced \"unknown\" values by np.NaN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save clenaed Datasets\r\n",
    "Note: using feather requires to reset the index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.reset_index().to_feather('df_azdias_cleaned_step1-cleaned')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\r\n",
    "Many records have ANZ_KINDER (number of children) = 0. \r\n",
    "For theses records the age of children columns (ALTER_KIND(N)) are always NaN. For records with a positive number \r\n",
    "of children the ALTER_KIND columns contains the age of children. We will replace these columns by summerize the\r\n",
    " them to two columns that will indicate the number of children younger than 10 and older or equal than 10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df  = df_azdias_cleaned\r\n",
    "cols_to_investigate = ['ALTER_KIND1','ALTER_KIND2','ALTER_KIND3','ALTER_KIND4','ANZ_KINDER']\r\n",
    "df_kinder = df[cols_to_investigate]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#df_kinder = df_azdias_cleaned.filter(regex='(ANZ_KINDER)|(ALTER_KIND.?)')\r\n",
    "\r\n",
    "figure, ax_list = plt.subplots(1,5,figsize=(24,5))\r\n",
    "\r\n",
    "for i, col in enumerate(cols_to_investigate):\r\n",
    "    df_kinder[col].value_counts().plot(kind='bar',ax=ax_list[i], title=col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Results**: The majority has no children. The dataset is quite imbalanced. Even the age of children is havily imbalanced. As you can see in chart `ALTER_KIND1` the distribution of ages has much higher values for >5 than for <=5\r\n",
    "\r\n",
    "Based on the observations above I will build a new feature `d_has_children` and `d_has_children_yte10` to indicate that person has children younger or equal than 10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Feature Engineering Process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import python.etl.processor as etlp\r\n",
    "\r\n",
    "featureBuilder = etlp.FeatureBuilder()\r\n",
    "df_azdias_cleaned = featureBuilder.transform(df_azdias_cleaned)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.info()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save results\r\n",
    "saving the results here will help to continue development and testing the next steps\r\n",
    "\r\n",
    "**Info**: [Best way to save pandas Dataframe](https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned.reset_index().to_feather('df_azdias_cleaned_step2-feaEngineered')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading DF\r\n",
    "you can start here if you want to skip steps before"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_azdias_cleaned = pd.read_feather('df_azdias_cleaned_step2-feaEngineered')\r\n",
    "\r\n",
    "# set the index as feather did store the index as column\r\n",
    "df_azdias_cleaned.set_index('LNR', inplace=True)\r\n",
    "df_azdias_cleaned.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Customer Segmentation Report\r\n",
    "\r\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cluster algorithms\r\n",
    "For clustering there is number of popular algorithms. For the algorthim selection I focused on the ones that scikit-learn provides and on the article [clustering algorithms with python](https://machinelearningmastery.com/clustering-algorithms-with-python/).\r\n",
    "\r\n",
    "## Feature Reduction and Selection\r\n",
    "\r\n",
    "The dimension of the dataset is quite high (442 features) so that it is worth to consider a reduction of the dimensionality which will increase the performance and in many cases the accuracy of algorithm. In particular the popular K-means which I will use will profit from it.\r\n",
    "\r\n",
    "See e.g. [PCA with k-means](https://365datascience.com/tutorials/python-tutorials/pca-k-means/)\r\n",
    "\r\n",
    "## Approach \r\n",
    "\r\n",
    "1. **Impute missing data**\r\n",
    "1. **Standardize data**\r\n",
    "\r\n",
    "1. **PCA - Principal Component Analysis**  \r\n",
    "This algorithms is also provided by scikit-learn. It will transform the given space of features to new space with basis vectors that are linear combinations of the given features so that the new vectors point in direction of the maximum variance.\r\n",
    "\r\n",
    "1. **K-means** \r\n",
    "\r\n",
    "\r\n",
    "For the complete process I will use a sklearn pipeline to chain the steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the pipeline steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "X_train = pd.read_feather('df_azdias_cleaned_step2-feaEngineered')\r\n",
    "\r\n",
    "# set the index as feather did store the index as column\r\n",
    "X_train.set_index('LNR', inplace=True)\r\n",
    "X_train.head()\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        AGER_TYP  AKT_DAT_KL  ALTER_HH  ALTER_KIND1  ALTER_KIND2  ALTER_KIND3  \\\n",
       "LNR                                                                             \n",
       "910215       NaN         NaN       NaN          NaN          NaN          NaN   \n",
       "910220       NaN         9.0       NaN          NaN          NaN          NaN   \n",
       "910225       NaN         9.0      17.0          NaN          NaN          NaN   \n",
       "910226       2.0         1.0      13.0          NaN          NaN          NaN   \n",
       "910241       NaN         1.0      20.0          NaN          NaN          NaN   \n",
       "\n",
       "        ALTER_KIND4  ALTERSKATEGORIE_FEIN  ANZ_HAUSHALTE_AKTIV  ANZ_HH_TITEL  \\\n",
       "LNR                                                                            \n",
       "910215          NaN                   NaN                  NaN           NaN   \n",
       "910220          NaN                  21.0                 11.0           0.0   \n",
       "910225          NaN                  17.0                 10.0           0.0   \n",
       "910226          NaN                  13.0                  1.0           0.0   \n",
       "910241          NaN                  14.0                  3.0           0.0   \n",
       "\n",
       "        ...  D19_LETZTER_KAUF_BRANCHE_D19_TELKO_MOBILE  \\\n",
       "LNR     ...                                              \n",
       "910215  ...                                          0   \n",
       "910220  ...                                          0   \n",
       "910225  ...                                          0   \n",
       "910226  ...                                          0   \n",
       "910241  ...                                          0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_TELKO_REST  \\\n",
       "LNR                                               \n",
       "910215                                        0   \n",
       "910220                                        0   \n",
       "910225                                        0   \n",
       "910226                                        0   \n",
       "910241                                        0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_TIERARTIKEL  \\\n",
       "LNR                                                \n",
       "910215                                         0   \n",
       "910220                                         0   \n",
       "910225                                         0   \n",
       "910226                                         0   \n",
       "910241                                         0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_UNBEKANNT  \\\n",
       "LNR                                              \n",
       "910215                                       0   \n",
       "910220                                       0   \n",
       "910225                                       1   \n",
       "910226                                       1   \n",
       "910241                                       0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_VERSAND_REST  \\\n",
       "LNR                                                 \n",
       "910215                                          0   \n",
       "910220                                          0   \n",
       "910225                                          0   \n",
       "910226                                          0   \n",
       "910241                                          0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_VERSICHERUNGEN  \\\n",
       "LNR                                                   \n",
       "910215                                            0   \n",
       "910220                                            0   \n",
       "910225                                            0   \n",
       "910226                                            0   \n",
       "910241                                            0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_VOLLSORTIMENT  \\\n",
       "LNR                                                  \n",
       "910215                                           0   \n",
       "910220                                           0   \n",
       "910225                                           0   \n",
       "910226                                           0   \n",
       "910241                                           0   \n",
       "\n",
       "        D19_LETZTER_KAUF_BRANCHE_D19_WEIN_FEINKOST  d_HAS_CHILDREN  \\\n",
       "LNR                                                                  \n",
       "910215                                           0               0   \n",
       "910220                                           0               0   \n",
       "910225                                           0               0   \n",
       "910226                                           0               0   \n",
       "910241                                           0               0   \n",
       "\n",
       "        d_HAS_CHILDREN_YTE10  \n",
       "LNR                           \n",
       "910215                     0  \n",
       "910220                     0  \n",
       "910225                     0  \n",
       "910226                     0  \n",
       "910241                     0  \n",
       "\n",
       "[5 rows x 442 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>ANZ_HH_TITEL</th>\n",
       "      <th>...</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TELKO_MOBILE</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TELKO_REST</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TIERARTIKEL</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_UNBEKANNT</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_VERSAND_REST</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_VERSICHERUNGEN</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_VOLLSORTIMENT</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_WEIN_FEINKOST</th>\n",
       "      <th>d_HAS_CHILDREN</th>\n",
       "      <th>d_HAS_CHILDREN_YTE10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LNR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910215</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910220</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910225</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910226</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910241</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 442 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "X_train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891221 entries, 910215 to 825787\n",
      "Columns: 442 entries, AGER_TYP to d_HAS_CHILDREN_YTE10\n",
      "dtypes: float64(300), int64(142)\n",
      "memory usage: 2.9 GB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imputation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\r\n",
    "\r\n",
    "X_train = imputer.fit_transform(X_train)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.93 GiB for an array with shape (442, 891221) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\STEPHA~1.BAU\\AppData\\Local\\Temp/ipykernel_23724/2832191386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'median'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             X = self._validate_data(X, reset=in_fit,\n\u001b[0m\u001b[0;32m    253\u001b[0m                                     \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                                     \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  10659\u001b[0m         \"\"\"\n\u001b[0;32m  10660\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10661\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10663\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecate_nonkeyword_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1464\u001b[0m                     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1467\u001b[0m             \u001b[1;31m# The underlying data was copied within _interleave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py39_dsnd\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         \u001b[1;31m# Tuple[Any, Union[int, Sequence[int]]], List[Any], _DTypeDict,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m         \u001b[1;31m# Tuple[Any, Any]]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1502\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[0mitemmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.93 GiB for an array with shape (442, 891221) and data type float64"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardization\r\n",
    "\r\n",
    "An important preprocessing step for PCA is stanardization (scaling) of the features. See [Importance of Feature Scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) for mor information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# in order to generalize we define a new variable for the dataset that will be used in the next steps\r\n",
    "df_segmentation = df_azdias_cleaned\r\n",
    "\r\n",
    "scaler = StandardScaler()\r\n",
    "X_train = scaler.fit_transform(df_segmentation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pca = PCA()\r\n",
    "pca.fit_transform(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train\r\n",
    "y_train \r\n",
    "# Fit to data and predict using pipelined scaling, GNB and PCA.\r\n",
    "std_clf = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='median'), StandardScaler(), PCA(n_components=2), GaussianNB())\r\n",
    "std_clf.fit(X_train, y_train)\r\n",
    "pred_test_std = std_clf.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Supervised Learning Model\r\n",
    "\r\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\r\n",
    "\r\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "interpreter": {
   "hash": "bd02d0b26d5bc5489eb8384059ce880300c63d4adcdc7615123473936c1c45dd"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('py39_dsnd': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}